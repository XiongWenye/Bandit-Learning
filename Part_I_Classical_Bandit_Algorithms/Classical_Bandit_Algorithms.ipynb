{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Classical Bandit Algorithms\n",
    "\n",
    "We consider a time-slotted bandit system $(t=1,2, \\ldots)$ with three arms. We denote the arm set as $\\{1,2,3\\}$. Pulling each arm $j(j \\in\\{1,2,3\\})$ will obtain a random reward $r_{j}$, which follows a Bernoulli distribution with mean $\\theta_{j}$, i.e., $\\operatorname{Bern}\\left(\\theta_{j}\\right)$. Specifically,\n",
    "\n",
    "$$\n",
    "r_{j}= \\begin{cases}1, & w \\cdot p \\cdot \\theta_{j} \\\\ 0, & w \\cdot p \\cdot 1-\\theta_{j}\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\theta_{j}, j \\in\\{1,2,3\\}$ are parameters within $(0,1)$.\n",
    "Now we run this bandit system for $N(N \\gg 3)$ time slots. In each time slot $t$, we choose one and only one arm from these three arms, which we denote as $I(t) \\in\\{1,2,3\\}$. Then we pull the arm $I(t)$ and obtain a random reward $r_{I(t)}$. Our objective is to find an optimal policy to choose an arm $I(t)$ in each time slot $t$ such that the expectation of the aggregated reward over $N$ time slots is maximized, i.e.,\n",
    "\n",
    "$$\n",
    "\\max _{I(t), t=1, \\ldots, N} \\mathbb{E}\\left[\\sum_{t=1}^{N} r_{I(t)}\\right]\n",
    "$$\n",
    "\n",
    "If we know the values of $\\theta_{j}, j \\in\\{1,2,3\\}$, this problem is trivial. Since $r_{I(t)} \\sim \\operatorname{Bern}\\left(\\theta_{I(t)}\\right)$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\sum_{t=1}^{N} r_{I(t)}\\right]=\\sum_{t=1}^{N} \\mathbb{E}\\left[r_{I(t)}\\right]=\\sum_{t=1}^{N} \\theta_{I(t)}\n",
    "$$\n",
    "\n",
    "Let $I(t)=I^{*}=\\arg \\max \\theta_{j}$ for $t=1,2, \\ldots, N$, then\n",
    "\n",
    "$$\n",
    "\\max _{I(t), t=1, \\ldots, N} \\mathbb{E}\\left[\\sum_{t=1}^{N} r_{I(t)}\\right]=N \\cdot \\theta_{I^{*}}\n",
    "$$\n",
    "\n",
    "However, in reality, we do not know the values of $\\theta_{j}, j \\in\\{1,2,3\\}$. We need to estimate the values $\\theta_{j}, j \\in\\{1,2,3\\}$ via empirical samples, and then make the decisions in each time slot. Next we introduce three classical bandit algorithms: $\\epsilon$-greedy, UCB, and TS, respectively.\n",
    "\n",
    "<img src=\"picture/greedy_UCB.png\" width=\"50%\" align='left'>\n",
    "<img src=\"picture/TS.png\" width=\"50%\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems 1  \n",
    "### Question  \n",
    "Now suppose we obtain the parameters of the Bernoulli distributions from an oracle, which are shown in the following table. Choose $N=5000$ and compute the theoretically maximized expectation of aggregate rewards over $N$ time slots. We call it the oracle value. Note that these parameters $\\theta_{j}, j \\in \\{1,2,3\\}$ and oracle values are unknown to all bandit algorithms.\n",
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Arm j</th>\n",
    "    <th>1</th>\n",
    "    <th>2</th>\n",
    "    <th>3</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Î¸<sub>j</td>\n",
    "    <td>0.7</td>\n",
    "    <td>0.5</td>\n",
    "    <td>0.4</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "### Solution\n",
    "Since each arm's parameter is known from the oracle, we need to choose the arm with the largest parameter to maximize the expectation of aggregate rewards over $N$ time slots.\n",
    "\n",
    "Given $\\theta_1 = 0.7, \\theta_2 = 0.5, \\theta_3 = 0.4$,\n",
    "we have $\\theta_1 > \\theta_2 > \\theta_3$.\n",
    "Thus, we choose arm 1 every time.\n",
    "\n",
    "i.e. $$\\forall t, I(t)=I^*=\\arg \\max\\limits_{j\\in\\{1,2,3\\}}\\theta_j=1$$\n",
    "$$\\theta_{I(t)} = \\theta_1 = 0.7$$\n",
    "\n",
    "Since $r_{I(t)} \\sim \\text{Bern}(\\theta_{I(t)})$,\n",
    "\n",
    "$$E(r_{I(t)}) = \\theta_{I(t)}$$\n",
    "\n",
    "The maximum expected value is \n",
    "$$\\max_{I(t),t=1,2,\\cdots,N}\\ E\\big[\\sum_{t=1}^Nr_{I(t)}\\big]$$\n",
    "$$=\\max_{I(t),t=1,2,\\cdots,N}\\ \\sum_{t=1}^NE\\big[r_{I(t)}\\big]$$\n",
    "$$=N \\cdot \\theta_{I^*} = 5000 \\times 0.7 = 3500$$\n",
    "\n",
    "Therefore, with the given oracle parameters, the maximum expected value is 3500.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2  \n",
    "### Question\n",
    "2. Implement classical bandit algorithms with following settings: \n",
    "   - $N=5000$\n",
    "   - $\\epsilon$-greedy with $\\epsilon \\in \\{0.1, 0.5, 0.9\\}$.\n",
    "   - UCB with $c \\in \\{1,5,10\\}$.\n",
    "   - TS with $\\left\\{(\\alpha_1,\\beta_1)=(1,1),(\\alpha_2,\\beta_2)=(1,1),(\\alpha_3,\\beta_3)=(1,1)\\right\\}$ and $\\left\\{(\\alpha_1,\\beta_1)=(601,401),(\\alpha_2,\\beta_2)=(401,601),(a3,b3)=(2,3)\\right\\}$\n",
    "\n",
    "### Solution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, theta=[0.7, 0.5, 0.4]):\n",
    "        self.theta = theta  \n",
    "        self.n_arms = len(theta)\n",
    "        self.counts = np.zeros(self.n_arms) \n",
    "        self.values = np.zeros(self.n_arms)  \n",
    "        \n",
    "    def pull(self, arm):\n",
    "        return np.random.binomial(1, self.theta[arm])\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        value = self.values[arm]\n",
    "        self.values[arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "class EpsilonGreedy(Bandit):\n",
    "    def __init__(self, epsilon, theta=[0.7, 0.5, 0.4]):\n",
    "        super().__init__(theta)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def select_arm(self):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)\n",
    "        return np.argmax(self.values)\n",
    "    \n",
    "    def modify_parameter(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "class UCB(Bandit):\n",
    "    def __init__(self, c, theta=[0.7, 0.5, 0.4]):\n",
    "        super().__init__(theta)\n",
    "        self.c = c\n",
    "        \n",
    "    def select_arm(self):\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        \n",
    "        total_counts = sum(self.counts)\n",
    "        ucb_values = self.values + self.c * np.sqrt(2 * np.log(total_counts) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def modify_parameter(self, c):\n",
    "        self.c = c\n",
    "\n",
    "class ThompsonSampling(Bandit):\n",
    "    def __init__(self, alpha=[1,1,1], beta=[1,1,1], theta=[0.7, 0.5, 0.4]):\n",
    "        super().__init__(theta)\n",
    "        self.alpha = np.array(alpha)\n",
    "        self.beta = np.array(beta)\n",
    "        \n",
    "    def select_arm(self):\n",
    "        samples = [np.random.beta(self.alpha[i], self.beta[i]) for i in range(self.n_arms)]\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        super().update(arm, reward)\n",
    "        self.alpha[arm] += reward\n",
    "        self.beta[arm] += (1 - reward)\n",
    "\n",
    "    def modify_parameter(self, alpha, beta):\n",
    "        self.alpha = np.array(alpha)\n",
    "        self.beta = np.array(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Each experiment lasts for N = 5000 time slots, and we run each experiment 200 trials. Results are averaged over these 200 independent trials.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.1   reward:  3412.44  regret:  86.41450000000351\n",
      "epsilon = 0.5   reward:  3079.025  regret:  418.2264999999955\n",
      "epsilon = 0.9   reward:  2753.085  regret:  749.9399999999831\n",
      "c = 1          reward:  3413.315  regret:  87.61050000003641\n",
      "c = 5          reward:  2978.395  regret:  519.4099999993863\n",
      "c = 10         reward:  2829.53  regret:  672.5744999995777\n",
      "alpha = [1,1,1] beta = [1,1,1] reward:  3483.01  regret:  18.015999999998755\n",
      "alpha = [601,401,2] beta = [401,601,3] reward:  3490.23  regret:  8.311499999999198\n"
     ]
    }
   ],
   "source": [
    "N = 5000 \n",
    "num_trials = 200 \n",
    "experiments = {\n",
    "    'epsilon_greedy': [0.1, 0.5, 0.9],  \n",
    "    'ucb': [1, 5, 10],  \n",
    "    'ts': [\n",
    "        ([1, 1, 1], [1, 1, 1]), \n",
    "        ([601, 401, 2], [401, 601, 3]) \n",
    "    ]\n",
    "}\n",
    "results_rewards = {\n",
    "    'epsilon_greedy': [],\n",
    "    'ucb': [],\n",
    "    'ts': []\n",
    "}\n",
    "results_regrets = {\n",
    "    'epsilon_greedy': [],\n",
    "    'ucb': [],\n",
    "    'ts': []\n",
    "}\n",
    "\n",
    "for key in ['epsilon_greedy', 'ucb', 'ts']:\n",
    "    for value in experiments[key]:\n",
    "        cumulative_rewards = 0\n",
    "        cumulative_regrets = 0\n",
    "        \n",
    "        for _ in range(num_trials):\n",
    "            if key == 'epsilon_greedy':\n",
    "                bandit = EpsilonGreedy(epsilon=value)\n",
    "            elif key == 'ucb':\n",
    "                bandit = UCB(c=value)\n",
    "            elif key == 'ts':\n",
    "                bandit = ThompsonSampling(alpha=value[0], beta=value[1])\n",
    "                \n",
    "            trial_rewards = []\n",
    "            for t in range(N):\n",
    "                arm = bandit.select_arm()\n",
    "                reward = bandit.pull(arm)\n",
    "                bandit.update(arm, reward)\n",
    "                cumulative_rewards += reward\n",
    "                regret = max(bandit.theta) - bandit.theta[arm]\n",
    "                cumulative_regrets += regret\n",
    "        results_rewards[key].append(cumulative_rewards / num_trials)\n",
    "        results_regrets[key].append(cumulative_regrets / num_trials)\n",
    "\n",
    "print(\"epsilon = 0.1   reward: \", results_rewards['epsilon_greedy'][0], \" regret: \", results_regrets['epsilon_greedy'][0])\n",
    "print(\"epsilon = 0.5   reward: \", results_rewards['epsilon_greedy'][1], \" regret: \", results_regrets['epsilon_greedy'][1])\n",
    "print(\"epsilon = 0.9   reward: \", results_rewards['epsilon_greedy'][2], \" regret: \", results_regrets['epsilon_greedy'][2])\n",
    "print(\"c = 1          reward: \", results_rewards['ucb'][0], \" regret: \", results_regrets['ucb'][0])\n",
    "print(\"c = 5          reward: \", results_rewards['ucb'][1], \" regret: \", results_regrets['ucb'][1])\n",
    "print(\"c = 10         reward: \", results_rewards['ucb'][2], \" regret: \", results_regrets['ucb'][2])\n",
    "print(\"alpha = [1,1,1] beta = [1,1,1] reward: \", results_rewards['ts'][0], \" regret: \", results_regrets['ts'][0])\n",
    "print(\"alpha = [601,401,2] beta = [401,601,3] reward: \", results_rewards['ts'][1], \" regret: \", results_regrets['ts'][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
