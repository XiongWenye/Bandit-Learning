{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Part I: Classical Bandit Algorithms\n",
    "\n",
    "We consider a time-slotted bandit system $(t=1,2, \\ldots)$ with three arms. We denote the arm set as $\\{1,2,3\\}$. Pulling each arm $j(j \\in\\{1,2,3\\})$ will obtain a random reward $r_{j}$, which follows a Bernoulli distribution with mean $\\theta_{j}$, i.e., $\\operatorname{Bern}\\left(\\theta_{j}\\right)$. Specifically,\n",
    "\n",
    "$$\n",
    "r_{j}= \\begin{cases}1, & w \\cdot p \\cdot \\theta_{j} \\\\ 0, & w \\cdot p \\cdot 1-\\theta_{j}\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\theta_{j}, j \\in\\{1,2,3\\}$ are parameters within $(0,1)$.\n",
    "Now we run this bandit system for $N(N \\gg 3)$ time slots. In each time slot $t$, we choose one and only one arm from these three arms, which we denote as $I(t) \\in\\{1,2,3\\}$. Then we pull the arm $I(t)$ and obtain a random reward $r_{I(t)}$. Our objective is to find an optimal policy to choose an arm $I(t)$ in each time slot $t$ such that the expectation of the aggregated reward over $N$ time slots is maximized, i.e.,\n",
    "\n",
    "$$\n",
    "\\max _{I(t), t=1, \\ldots, N} \\mathbb{E}\\left[\\sum_{t=1}^{N} r_{I(t)}\\right]\n",
    "$$\n",
    "\n",
    "If we know the values of $\\theta_{j}, j \\in\\{1,2,3\\}$, this problem is trivial. Since $r_{I(t)} \\sim \\operatorname{Bern}\\left(\\theta_{I(t)}\\right)$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\sum_{t=1}^{N} r_{I(t)}\\right]=\\sum_{t=1}^{N} \\mathbb{E}\\left[r_{I(t)}\\right]=\\sum_{t=1}^{N} \\theta_{I(t)}\n",
    "$$\n",
    "\n",
    "Let $I(t)=I^{*}=\\arg \\max \\theta_{j}$ for $t=1,2, \\ldots, N$, then\n",
    "\n",
    "$$\n",
    "\\max _{I(t), t=1, \\ldots, N} \\mathbb{E}\\left[\\sum_{t=1}^{N} r_{I(t)}\\right]=N \\cdot \\theta_{I^{*}}\n",
    "$$\n",
    "\n",
    "However, in reality, we do not know the values of $\\theta_{j}, j \\in\\{1,2,3\\}$. We need to estimate the values $\\theta_{j}, j \\in\\{1,2,3\\}$ via empirical samples, and then make the decisions in each time slot. Next we introduce three classical bandit algorithms: $\\epsilon$-greedy, UCB, and TS, respectively.\n",
    "\n",
    "![](greedy_UCB.png)\n",
    "![](TS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
