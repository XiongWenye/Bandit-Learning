{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 51.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.1, alpha: [1, 1], beta: [1, 1], avg_reward: 0.5813, avg_regret_rate: 0.2141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 51.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.1, alpha: [2, 1], beta: [1, 1], avg_reward: 0.5480, avg_regret_rate: 0.2491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 51.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.1, alpha: [20, 1], beta: [10, 1], avg_reward: 0.5822, avg_regret_rate: 0.2060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 51.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.3, alpha: [1, 1], beta: [1, 1], avg_reward: 0.7785, avg_regret_rate: 0.1925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 50.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.3, alpha: [2, 1], beta: [1, 1], avg_reward: 0.7315, avg_regret_rate: 0.1923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 52.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.3, alpha: [20, 1], beta: [10, 1], avg_reward: 0.7297, avg_regret_rate: 0.2278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.5, alpha: [1, 1], beta: [1, 1], avg_reward: 1.0941, avg_regret_rate: 0.1806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.5, alpha: [2, 1], beta: [1, 1], avg_reward: 1.0408, avg_regret_rate: 0.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 50.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.5, alpha: [20, 1], beta: [10, 1], avg_reward: 0.9554, avg_regret_rate: 0.2816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 50.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.7, alpha: [1, 1], beta: [1, 1], avg_reward: 1.9450, avg_regret_rate: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 50.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.7, alpha: [2, 1], beta: [1, 1], avg_reward: 1.7140, avg_regret_rate: 0.1933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 51.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.7, alpha: [20, 1], beta: [10, 1], avg_reward: 1.6554, avg_regret_rate: 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 49.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.9, alpha: [1, 1], beta: [1, 1], avg_reward: 6.0248, avg_regret_rate: 0.1107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 51.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.9, alpha: [2, 1], beta: [1, 1], avg_reward: 5.9431, avg_regret_rate: 0.1089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.9, alpha: [20, 1], beta: [10, 1], avg_reward: 5.0471, avg_regret_rate: 0.1986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.99, alpha: [1, 1], beta: [1, 1], avg_reward: 64.2234, avg_regret_rate: 0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 51.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.99, alpha: [2, 1], beta: [1, 1], avg_reward: 64.7984, avg_regret_rate: 0.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 52.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.99, alpha: [20, 1], beta: [10, 1], avg_reward: 60.8028, avg_regret_rate: 0.1167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def intuitive_policy(N, gamma, true_theta, alpha, beta):\n",
    "    \"\"\"\n",
    "    Implements the intuitive policy for a two-armed bandit problem with discounted rewards.\n",
    "    \n",
    "    Args:\n",
    "        N (int): Number of time steps\n",
    "        gamma (float): Discount factor\n",
    "        true_theta (np.ndarray): True probabilities for each arm\n",
    "        alpha (list): Initial alpha parameters for Beta distribution\n",
    "        beta (list): Initial beta parameters for Beta distribution\n",
    "    \n",
    "    Returns:\n",
    "        float: Sum of discounted rewards\n",
    "    \"\"\"\n",
    "    rewards = np.zeros(N)\n",
    "    alpha = np.array(alpha)  \n",
    "    beta = np.array(beta)\n",
    "\n",
    "    for t in range(N):\n",
    "        theta_estimate = alpha / (alpha + beta)\n",
    "        chosen_arm = np.argmax(theta_estimate)\n",
    "\n",
    "        reward = np.random.rand() < true_theta[chosen_arm]\n",
    "        rewards[t] = reward * (gamma ** t)\n",
    "        \n",
    "        alpha[chosen_arm] += reward\n",
    "        beta[chosen_arm] += 1 - reward\n",
    "\n",
    "    return np.sum(rewards)\n",
    "\n",
    "num_trials = 200\n",
    "N = 5000\n",
    "gamma_list = [0.1, 0.3, 0.5, 0.7, 0.9, 0.99]\n",
    "alpha_list = [[1, 1], [2, 1], [20, 1]]\n",
    "beta_list = [[1, 1], [1, 1], [10, 1]]\n",
    "\n",
    "for gamma in gamma_list:\n",
    "    for alpha, beta in zip(alpha_list, beta_list):\n",
    "        rewards = np.zeros(num_trials)\n",
    "        regret_rate = np.zeros(num_trials)\n",
    "        \n",
    "        for i in tqdm(range(num_trials)):\n",
    "            true_theta = np.random.rand(2)\n",
    "\n",
    "            rewards[i] = intuitive_policy(N, gamma, true_theta, alpha, beta)\n",
    "            max_value = np.max(true_theta) / (1 - gamma)\n",
    "            regret_rate[i] = 1 - rewards[i] / max_value\n",
    "            \n",
    "        print(f\"gamma: {gamma}, alpha: {alpha}, beta: {beta}, \"\n",
    "              f\"avg_reward: {np.mean(rewards):.4f}, \"\n",
    "              f\"avg_regret_rate: {np.mean(regret_rate):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of the algorithm, we need to find a suitable metric. Regret seems to be a good choice, but it is not normalized, leading to different scales for different settings. (For example, larger $\\gamma$ leads to larger regret.) Thus, we use the regret rate, which shows the portion of regret to the maximum expected reward.   \n",
    "The regret rate is defined as follow:  \n",
    "\n",
    "$\\text{regret rate} = 1 - \\frac{Reward}{\\max_i \\theta_i/(1-\\gamma)}$\n",
    "\n",
    "where the maximum possible reward is achieved by always pulling the arm with the highest true probability. For the discounted setting, this equals $\\frac{\\max_i \\theta_i}{1-\\gamma}$.\n",
    "\n",
    "The simulation results show that the intuitive policy performs well in most cases, achieving low regret rates.  \n",
    "\n",
    "| γ | Prior (α, β) | Average Reward | Average Regret Rate |\n",
    "|---|-------------|----------------|-------------------|\n",
    "| 0.1 | [1,1], [1,1] | 0.5813 | 0.2141 |\n",
    "| 0.1 | [2,1], [1,1] | 0.5480 | 0.2491 |\n",
    "| 0.1 | [20,1], [10,1] | 0.5822 | 0.2060 |\n",
    "| 0.3 | [1,1], [1,1] | 0.7785 | 0.1925 |\n",
    "| 0.3 | [2,1], [1,1] | 0.7315 | 0.1923 |\n",
    "| 0.3 | [20,1], [10,1] | 0.7297 | 0.2278 |\n",
    "| 0.5 | [1,1], [1,1] | 1.0941 | 0.1806 |\n",
    "| 0.5 | [2,1], [1,1] | 1.0408 | 0.2142 |\n",
    "| 0.5 | [20,1], [10,1] | 0.9554 | 0.2816 |\n",
    "| 0.7 | [1,1], [1,1] | 1.9450 | 0.0872 |\n",
    "| 0.7 | [2,1], [1,1] | 1.7140 | 0.1933 |\n",
    "| 0.7 | [20,1], [10,1] | 1.6554 | 0.2925 |\n",
    "| 0.9 | [1,1], [1,1] | 6.0248 | 0.1107 |\n",
    "| 0.9 | [2,1], [1,1] | 5.9431 | 0.1089 |\n",
    "| 0.9 | [20,1], [10,1] | 5.0471 | 0.1986 |\n",
    "| 0.99 | [1,1], [1,1] | 64.2234 | 0.0325 |\n",
    "| 0.99 | [2,1], [1,1] | 64.7984 | 0.0525 |\n",
    "| 0.99 | [20,1], [10,1] | 60.8028 | 0.1167 |\n",
    "\n",
    "This can be attributed to several factors:\n",
    "\n",
    "1. **Efficient Exploration**: The policy naturally balances exploration and exploitation through Bayesian updating of the Beta distributions.\n",
    "\n",
    "2. **Prior Knowledge Integration**: The Beta distribution parameters (α, β) allow incorporating prior knowledge about the arms, which helps guide initial exploration.\n",
    "\n",
    "3. **Quick Convergence**: As more rewards are observed, the posterior distributions quickly concentrate around the true probabilities, leading to optimal arm selection.\n",
    "\n",
    "Looking at the simulation results across different discount factors (γ) and prior parameters (α, β), we see consistently low regret rates, indicating the policy's robustness to different parameter settings. However, as we'll see in the counter-example, there are specific scenarios where this policy can be suboptimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 37.93\n",
      "Maximum possible reward: 80.00\n",
      "Regret rate: 0.53\n"
     ]
    }
   ],
   "source": [
    "# Here's a counter-example with a strongly biased prior, but the second arm is actually better.\n",
    "# We'll run fewer steps (N_short=100), so the policy doesn't have time to correct the prior bias.\n",
    "\n",
    "test_runs = 1000\n",
    "test_rewards = np.zeros(test_runs)\n",
    "N_short = 100\n",
    "gamma_close_to_1 = 0.99\n",
    "\n",
    "\n",
    "counter_alpha = [200, 1]\n",
    "counter_beta = [1, 1]\n",
    "counter_true_theta = np.array([0.6, 0.8])  # second arm has higher probability\n",
    "\n",
    "for i in range(test_runs):\n",
    "    local_alpha = counter_alpha.copy()\n",
    "    local_beta = counter_beta.copy()\n",
    "    test_rewards[i] = intuitive_policy(N_short, gamma_close_to_1, counter_true_theta, local_alpha, local_beta)\n",
    "\n",
    "avg_r = np.mean(test_rewards)\n",
    "optimal_r = np.max(counter_true_theta) / (1 - gamma_close_to_1)\n",
    "print(f\"Average reward: {avg_r:.2f}\")\n",
    "print(f\"Maximum possible reward: {optimal_r:.2f}\")\n",
    "print(f\"Regret rate: {1 - avg_r/optimal_r:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two arms with prior distributions:  \n",
    "#### Arm1  \n",
    "Beta(200,1), suggesting an expected value close to 1.0\n",
    "#### Arm2\n",
    "Beta(1,1), suggesting an expected value close to 0.5\n",
    "\n",
    "A greedy strategy that consistently favors the arm with the higher expected value may lead to repeatedly selecting Arm 1. However, this approach has critical flaws. The prior for Arm 2 suggests significant uncertainty, as the Beta(1, 1) distribution is essentially non-informative, assigning equal probability to all values between 0 and 1.\n",
    "\n",
    "By selecting Arm 2 more frequently, we can reduce this uncertainty and potentially uncover a true value for Arm 2 that exceeds that of Arm 1.\n",
    "\n",
    "Focusing exclusively on Arm 1 due to its higher initial expected value neglects the possibility that Arm 2 could ultimately provide greater rewards once more data is collected. Failing to explore Arm 2 adequately risks missing out on higher returns that could arise if its true value is found to be higher than initially estimated.\n",
    "\n",
    "When priors differ significantly in terms of uncertainty, a strategy that relies solely on expected values can lead to consistently selecting a suboptimal arm. It is crucial to strike a balance between exploiting known information and exploring uncertain but potentially more rewarding alternatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
